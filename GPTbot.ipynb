{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7pix3HJRvaqcjfVtlKAqE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mehul-Agrawal410/Mini-Shakespeare-Chatbot/blob/main/GPTbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "DOiiWw8HJ76S"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWa9N9gMwIvD",
        "outputId": "a6eb04ea-5935-4935-cb49-00e3b33252dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-15 10:50:43--  https://github.com/Mehul-Agrawal410/Mini-Shakespeare-Chatbot/blob/main/input.txt\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4276 (4.2K) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   4.18K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-07-15 10:50:43 (39.6 MB/s) - ‘input.txt’ saved [4276/4276]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/Mehul-Agrawal410/Mini-Shakespeare-Chatbot/blob/main/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(\"length of dataset in characters: \", len(text))\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpf_Tw8u0qeJ",
        "outputId": "e75d3cf2-1f50-4d1a-f535-fdd4f51b46d7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(chars, vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFKUXPuj02L7",
        "outputId": "59063c42-3430-4764-e583-10fd2b98f7d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ch_to_ix = {c:i for i, c in enumerate(chars)}\n",
        "ix_to_ch  = {i:c for i,c in enumerate(chars)}\n",
        "encode = lambda s: [ch_to_ix[i] for i in s]\n",
        "decode = lambda s: ''.join([ix_to_ch[i] for i in s])"
      ],
      "metadata": {
        "id": "GAWmCons1j-S"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = encode('hello there')\n",
        "print(a)\n",
        "print(''.join(decode(a)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JWnscPh2QjR",
        "outputId": "52a289f2-4081-4a0e-f234-8533fdbf1fd2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43]\n",
            "hello there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8031ViJ2tHH",
        "outputId": "91543a33-e524-4850-be8f-9309d38c37d5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data))\n",
        "train = data[:n]\n",
        "val = data[n:]"
      ],
      "metadata": {
        "id": "YOVioqPY_6M0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "block_size = 16\n",
        "max_iters = 3000\n",
        "eval_interval = 300  #just for calculating loss at fixedd intervals\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "num_embd = 64   #embedding size of every unique character in vocabulary"
      ],
      "metadata": {
        "id": "7Uux3DQX3-lB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(data):\n",
        "  indexes = torch.randint(high = len(data) - block_size, size = (batch_size, ))\n",
        "  x = torch.stack([data[i:i+block_size] for i in indexes])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in indexes])\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "HGuKcaPfASTl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_batch, y_batch = get_batch(train)\n",
        "print(x_batch, y_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzjfuzjmB7RO",
        "outputId": "bbe3bee4-e1fb-4a86-9dd6-c5ea4f26781a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1, 39,  1, 61, 53, 56, 42,  1, 61, 47, 58, 46,  1, 53, 52, 43],\n",
            "        [53, 59, 56,  5, 42,  1, 44, 53, 56,  1, 46, 47, 57,  1, 40, 59],\n",
            "        [46, 47, 51, 10,  0, 28, 56, 39, 63,  1, 19, 53, 42,  1, 61, 43],\n",
            "        [43, 40, 63,  6,  0, 14, 43, 41, 39, 59, 57, 43,  1, 21,  1, 61],\n",
            "        [47, 51, 43,  7, 54, 50, 43, 39, 57, 43, 56, 57,  6,  1, 44, 50],\n",
            "        [39, 56,  1, 51, 43,  6,  1, 52, 53, 40, 50, 43,  1, 50, 53, 56],\n",
            "        [61, 43,  1, 61, 47, 50, 50,  1, 54, 56, 53, 41, 50, 39, 47, 51],\n",
            "        [47, 52, 43,  1, 53, 61, 52,  1, 45, 53, 53, 42,  1, 61, 47, 50]]) tensor([[39,  1, 61, 53, 56, 42,  1, 61, 47, 58, 46,  1, 53, 52, 43,  1],\n",
            "        [59, 56,  5, 42,  1, 44, 53, 56,  1, 46, 47, 57,  1, 40, 59, 56],\n",
            "        [47, 51, 10,  0, 28, 56, 39, 63,  1, 19, 53, 42,  1, 61, 43,  1],\n",
            "        [40, 63,  6,  0, 14, 43, 41, 39, 59, 57, 43,  1, 21,  1, 61, 47],\n",
            "        [51, 43,  7, 54, 50, 43, 39, 57, 43, 56, 57,  6,  1, 44, 50, 39],\n",
            "        [56,  1, 51, 43,  6,  1, 52, 53, 40, 50, 43,  1, 50, 53, 56, 42],\n",
            "        [43,  1, 61, 47, 50, 50,  1, 54, 56, 53, 41, 50, 39, 47, 51,  1],\n",
            "        [52, 43,  1, 53, 61, 52,  1, 45, 53, 53, 42,  1, 61, 47, 50, 50]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLangModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, num_embd)\n",
        "    self.langModel_head = nn.Linear(num_embd, vocab_size)\n",
        "\n",
        "  def forward(self, index, targets = None):\n",
        "\n",
        "    batch, seq_len = index.shape\n",
        "    #index and target are (batch_size, seq_len) tensors\n",
        "    tok_embd = self.token_embedding_table(index)  #(batch_size, seq_len, emb_size)\n",
        "    logits = self.langModel_head(tok_embd)  #(batch_size, seq_len, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      batch_size, seq_len, emb_size = logits.shape\n",
        "      logits = logits.view(batch_size * seq_len, emb_size)\n",
        "      targets = targets.view(batch_size * seq_len)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  #extends the time dimension by max tokens\n",
        "  def generate(self, context, max_new_tokens):\n",
        "    #context is (batch, time) array of indices in current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(context)\n",
        "      logits = logits[:,-1,:] # pluck out the last element in \"time\" as that will predict next elem\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      index_next = torch.multinomial(probs, num_samples = 1) #Returns num_samples rows indices sampled from the multinomial probability \"probs\"\n",
        "      context = torch.cat([context, index_next], dim=1) #(batch, time+1)\n",
        "    return context\n"
      ],
      "metadata": {
        "id": "aLcUOufMCNVi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLangModel()\n",
        "raw_outs, loss = model(x_batch, y_batch)\n",
        "print(raw_outs.shape)\n",
        "print(loss)\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long)\n",
        "print(decode(model.generate(context, max_new_tokens=100)[0].tolist())) #0 cause it return (1,1) tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jSoIByarImJ",
        "outputId": "20cc6231-769f-41ec-a13f-82e29e313652"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 65])\n",
            "tensor(4.2876, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "jdmAj?hanBevKE3DL;$-PDxZAhERxx BL'Spc:SypDuDChnWz.u;TJzRaS:a-fV$c'pkKn'E;e3IM&,:kVzAatkii,xzaFTScH$M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "beOWnWyOM6Al"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in [train, val]:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "eiGjeJwq4PkY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses[train]:.4f}, val loss {losses[val]:.4f}\")\n",
        "\n",
        "  # sample a batch of data\n",
        "  x_batch, y_batch = get_batch(train)\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = model(x_batch, y_batch)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "id": "xeywbiTbeW18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a3b50bc-653c-4af0-f8b8-d8aea6626964"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3577, val loss 4.3675\n",
            "step 300: train loss 2.7507, val loss 2.7505\n",
            "step 600: train loss 2.5965, val loss 2.6071\n",
            "step 900: train loss 2.5299, val loss 2.5649\n",
            "step 1200: train loss 2.5182, val loss 2.5466\n",
            "step 1500: train loss 2.5123, val loss 2.5347\n",
            "step 1800: train loss 2.5063, val loss 2.5296\n",
            "step 2100: train loss 2.4848, val loss 2.5170\n",
            "step 2400: train loss 2.4838, val loss 2.5288\n",
            "step 2700: train loss 2.4983, val loss 2.5174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(model.generate(torch.zeros((1,1), dtype = torch.long), 2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrbXTSWCv__E",
        "outputId": "66c166a9-ec97-49c8-dbe8-0cd6b4be08fe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Thn mellif an o l a mouny ickery r aasinCKEd y t ingnnd e;\n",
            "Thepe tthy, tongerot ths go VI sskipr igo: t'd t str tantwisarilot bath ce alamary ilghes, RI gou o athif t cisuriemathtet mm, outy, y, heay thatares y acem\n",
            "\n",
            "t ourout th shof crer llim?\n",
            "FCllave-pas:\n",
            "Ant'vet and\n",
            "Forear, Haioures aspe,\n",
            "Tisor f D:\n",
            "ND is helel h T:\n",
            "TENICiofathaindeage d ar, ty hopy, da atayord hesuthike; ororale, f lu wonthemome I mese wo is sod.\n",
            "Gor s s ay h ickad, t ad be thehe, atisoult!bo thasullon'ds'ssinthar:\n",
            "Nong; nd by, we y y nd h\n",
            "3-r s;\n",
            "Me my? wean?\n",
            "Habr herendot,\n",
            "ANTENDsapow by Ache'T:\n",
            "Thidy thar bran incrothink thas, blersuce cas.\n",
            "\n",
            "F\n",
            "I:\n",
            "Aner on, gan bor;\n",
            "W t ch s yiffotrt EO:\n",
            "Looustha meanof inds t: we chWhathar o s s\n",
            "GLyothar .\n",
            "\n",
            "fot, be IOLuseristhinof me.\n",
            "INGy o croliningad p\n",
            "We thrt d s mp hin'ta miclspougoune ds, whendaurertrary h, ss tor y velou tet jcke tome thugrr g'eande, brt, lnowity EN e g!\n",
            "TELThth;\n",
            "Imy, ikea co t tor h be\n",
            "Anghinds owh m tunathat ss; y; ond:\n",
            "Thenire lesse we f wht l vene me sllf m,\n",
            "IS:\n",
            "ABure m VIN higrghe f sth pothy ave g w co y s oos;\n",
            "wits in bRDWimet.\n",
            "Tha orica the Roressinthong t d t methe theanth st the'oughant hen corst dondind odOMIRie m andre the Ibj dl s\n",
            "patos houthyotrecoo houco t be g nd s: mbuke ngoueag by th's, anker'dl aroryod, ighe,\n",
            "\n",
            "Soou. if'st prele;\n",
            "Ifld aminethe t ve ar pous I sorary h oth anper, sec- loo s n:\n",
            "He hexing at ivengly bur o hered orecadikswo s rertailofome w meintobof I:\n",
            "RESace bonlip jofo thatok a m ar?\n",
            "Mod cer; gusw aloseatUSitrthe toyo Bue g tegy alopar:,\n",
            "Angeilllot hryoruround tingr foffafuthounss LOLESAnd mofossaker pll toruthen ssullst EMESin ceake akell yon myas th lofor, wie ibe by me inke. and sos whomoraldit s beleminckigut ay, aromom t foF fou, d narer mar tsioushapenttouronebe d tsucr wo s thee hy het mer rlointardsteer d he d ean!\n",
            "BRIO: t ho f thary; wed th iso midELou&MES:\n",
            "Bk d fe BUSIs t t ke, s youomyor.\n",
            "Tam ithed o cand inay s e OFre n h, Vg f ted ie!\n",
            "DIf ily d fores me be by wof\n",
            "OLUDru t sple he,\n",
            "NGLIthend n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300  #just for calculating loss at fixedd intervals\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "num_embd = 32   #embedding size of every unique character in vocabulary\n",
        "num_head = 4\n",
        "num_layer = 4  #attention hyperparameter\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "63ZtXcwILiy2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(num_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(num_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(num_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # input of size (batch, time-step, channels)\n",
        "    # output of size (batch, time-step, head size)\n",
        "    batch_size, seq_len, emb_size = x.shape\n",
        "    k = self.key(x)                                       # (batch_size, seq_len, head_size)\n",
        "    q = self.query(x)                                     # (batch_size, seq_len, head_size)\n",
        "    wei =  q @ k.transpose(-2, -1) * k.shape[-1]**-0.5    # (batch_size, seq_len, head_size) @ (batch_size, head_size, seq_len) ---> (batch_size, seq_len, seq_len)\n",
        "                                                          # this outputs the raw affinity between all the nodes\n",
        "    tril = torch.tril(torch.ones(seq_len, seq_len))\n",
        "    wei = wei.masked_fill(self.tril == 0, float('-inf'))       #because we don't want the affinity with nodes ahead of current node\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out"
      ],
      "metadata": {
        "id": "6vOKiD9dczto"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(head_size * num_heads, num_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out"
      ],
      "metadata": {
        "id": "NPkq9cCSMmom"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "\n",
        "  def __init__(self, num_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(num_embd, 4 * num_embd),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(4 * num_embd, num_embd),\n",
        "      nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "2IqQiTG4Mvdw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "  def __init__(self, num_embd, num_head):\n",
        "    # num_embd: embedding dimension, num_head: the number of heads we'd like\n",
        "    super().__init__()\n",
        "    head_size = num_embd // num_head\n",
        "    self.self_attn = MultiHeadAttention(num_head, head_size)\n",
        "    self.ffwd = FeedFoward(num_embd)\n",
        "    self.layer_norm_1 = nn.LayerNorm(num_embd)\n",
        "    self.layer_norm_2 = nn.LayerNorm(num_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.self_attn(self.layer_norm_1(x))\n",
        "    x = x + self.ffwd(self.layer_norm_2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "tgMdZlMqNADU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, num_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, num_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(num_embd, num_head=num_head) for _ in range(num_layer)])\n",
        "    self.ln_f = nn.LayerNorm(num_embd) # final layer norm\n",
        "    self.lm_head = nn.Linear(num_embd, vocab_size)\n",
        "\n",
        "    # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "      if module.bias is not None:\n",
        "          torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    batch_size, seq_len = idx.shape\n",
        "\n",
        "    # idx and targets are both (batch_size,seq_len) tensor of integers\n",
        "    tok_emb = self.token_embedding_table(idx) # (batch_size,seq_len,token_embd_size)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(seq_len, device=device)) # (seq_len, pos_embd_size)\n",
        "    x = tok_emb + pos_emb # (batch_size,seq_len,C)\n",
        "    x = self.blocks(x) # (batch_size,seq_len,C)\n",
        "    x = self.ln_f(x) # (batch_size,seq_len,C)\n",
        "    logits = self.lm_head(x) # (batch_size,seq_len,vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      batch_size, seq_len, embd_size = logits.shape\n",
        "      logits = logits.view(batch_size*seq_len, embd_size)\n",
        "      targets = targets.view(batch_size*seq_len)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, context, max_new_tokens):\n",
        "    # context is (batch_size, seq_len) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop idx to the last block_size tokens\n",
        "      context_cropped = context[:, -block_size:]\n",
        "      # get the predictions\n",
        "      logits, loss = self(context_cropped)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[:, -1, :] # becomes (batch_size, C)\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (batch_size, C)\n",
        "      # sample from the distribution\n",
        "      context_next = torch.multinomial(probs, num_samples=1) # (batch_size, 1)\n",
        "      # append sampled index to the running sequence\n",
        "      context = torch.cat((context, context_next), dim=1) # (batch_size, seq_len+1)\n",
        "    return context"
      ],
      "metadata": {
        "id": "uagRGF6nNs_C"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters()), 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ8RNcj9R1oR",
        "outputId": "b0b475c0-17f5-490b-b873-f033ea048bcd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54977 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "_F-CznlaR2lV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in [train, val]:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "LoVDPpsIVjZc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "  # every once in a while evaluate the loss on train and val sets\n",
        "  if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses[train]:.4f}, val loss {losses[val]:.4f}\")\n",
        "\n",
        "  # sample a batch of data\n",
        "  x_batch, y_batch = get_batch(train)\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = model(x_batch, y_batch)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jV8r5_pR8wN",
        "outputId": "f67e1585-8fbc-46bb-96dc-f5db4b1453a0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1738, val loss 4.1739\n",
            "step 300: train loss 2.5078, val loss 2.5100\n",
            "step 600: train loss 2.3894, val loss 2.3949\n",
            "step 900: train loss 2.3319, val loss 2.3225\n",
            "step 1200: train loss 2.2694, val loss 2.2806\n",
            "step 1500: train loss 2.2338, val loss 2.2496\n",
            "step 1800: train loss 2.1824, val loss 2.2049\n",
            "step 2100: train loss 2.1620, val loss 2.1765\n",
            "step 2400: train loss 2.1486, val loss 2.1685\n",
            "step 2700: train loss 2.1343, val loss 2.1585\n",
            "step 2999: train loss 2.1142, val loss 2.1504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 8), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHENyn6gSHSb",
        "outputId": "0d60c2a5-0443-4600-ea09-ad0ff24a1cfe"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "HEBETUME: Is gor it ad, cAg war rould creaie, your ther'se a the etl thond you, to the mstowns, trakt\n",
            "Tho, we to do sieck.\n",
            "\n",
            "\n",
            "DEOLUS:\n",
            "Pry, lealle fert,\n",
            "Hiswith sbonboone difests.\n",
            "BRIAR:\n",
            "As mres; then he wall belorgue fatter wil thablan thy wo lorce,\n",
            "On wo we buth\n",
            "As grovem lol fridon ; With! sto It:\n",
            "\n",
            "The thast befZanes cee meenot, the hink what are coman hall\n",
            "To kn to akeplend lark prathever ay?\n",
            "\n",
            "RAPVNRAREOKA:\n",
            "Booone\n",
            "A Of and.\n",
            "I foris eor my liey, for nou hovo wheld\n",
            "it of may, my my tan ut thy dee tho is are wilxfet I law; the thar?\n",
            "\n",
            "And with shan grest'somy oun mined mak my quue stk blaing, thy brothere et coy, as cre ce:\n",
            "sthis I qunessbe vithill.\n",
            "RAVERICICETUCHARYNIV:\n",
            "Nouge\n",
            "Porure hourn and why,\n",
            "So on sacuteen'drs of wall.\n",
            "VARIBETERS:\n",
            "A tha ach love thour daine blatif gook! wre mith graie, thy Gostine.\n",
            "\n",
            "CORIVEN\n",
            "Chat pous thou a tha Lot:\n",
            "Harth on'gmat thur Romfers\n",
            "ford pose Soustirs in of that, the ase stive gesing,\n",
            "Cie?\n",
            "\n",
            "\n",
            "Sith,\n",
            "VINGIERS:\n",
            "Lod.\n",
            "\n",
            "GLOscitk:\n",
            "Tho till sive dind bre ay, thirt, deay te,\n",
            "Womphay\n",
            "pet tose troud! swith.\n",
            "\n",
            "Do fost ronkie min my Gove here forret it to sure. Caurett an o fuchby in work fave grelt, the upe it\n",
            "sive thatur hine it charce my agly, no eds, ut in comontly!\n",
            "Is sthoum we lomy\n",
            "doow ton whe make far will,\n",
            "The thacke shall matiure ar thou gher, fre he sho; igh thou xnowed a for priply! I,\n",
            "Buchy If arlark this my: cord, hen, Gort learce, mpeites fire an fay stirst, swearl lay wag;\n",
            "BEltes, wha\n",
            "By ore.\n",
            "And:\n",
            "He picert thinsh,\n",
            "And Eve.\n",
            "\n",
            "NUFUTEIUMPLENTEO:\n",
            "If hum betian bsed out ne, is, t hengenis thy mow nkie the not the nimonge, roooS, are cksoo, a fullat elly.\n",
            "\n",
            "To that I whed traul'and hordutely\n",
            "And of lets that gath:\n",
            "Hee?\n",
            "\n",
            "\n",
            "SHEORDY ENENCUD:\n",
            "KING bunt.\n",
            "ETK:\n",
            "Man?\n",
            "\n",
            "\n",
            "II\n",
            "Can's we licte anfes and one loy hert pruchstentior yuse tur acroum hear whis bartonry, a milt. mothen it the cortese, to chivery, cong wereath sit; comfforrow my treeseakis!\n",
            "KINGNY Prove ver he wastlewire, in is wither.\n",
            "\n",
            "BOngut Tim be fet te, maniny, teid, for it akir, sow ur fore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open('outputs.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSNqXVYFxZMp",
        "outputId": "79442966-bf2f-4a79-cde0-428c8c70a631"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10008"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}